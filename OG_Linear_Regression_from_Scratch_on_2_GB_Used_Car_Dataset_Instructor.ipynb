{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akansha2818/pythonprograms/blob/main/OG_Linear_Regression_from_Scratch_on_2_GB_Used_Car_Dataset_Instructor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehqeDcenY3M5"
      },
      "source": [
        "#**This Notebook has been curated by Axis India Machine Learning (AiML)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g4VXi_z8Y-lE"
      },
      "source": [
        "![](https://drive.google.com/uc?export=view&id=1P38a0NLCj_NB52a6us_YPSkt6U3N__EL)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tcDQ35rYZDcO"
      },
      "source": [
        "#**To know more about us, you can visit [here](https://www.aimlrl.com)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QLMjmnd4lsqW"
      },
      "source": [
        "#In this mini project, **we will be training a Linear Regression model to perform accurate prediction of Price (Target Feature, $y$) of Used Cars, using the dataset of Used Cars having 25 Input Features ($X$) and 1 Target Feature of Price, available [here](https://www.kaggle.com/datasets/austinreese/craigslist-carstrucks-data).**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HGL8Mi64nmWp"
      },
      "source": [
        "#In this dataset, we will be having **data of 426880 Cars, each having initially an Input Feature Vector of 25 Dimensions which is a Row Vector ($\\vec{x}^i$) and a scalar Target Feature ($y_i$).**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FH7VWC4QoKyN"
      },
      "source": [
        "#Let's download the data from Kaggle."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x60a6XarCJIj"
      },
      "outputs": [],
      "source": [
        "from google.colab import files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "qGzOjdA1Fs9S",
        "outputId": "d535d1d3-8ec3-438f-867e-4c3054039811"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-158b614c-aa1e-4f62-b1d6-292d43d640e8\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-158b614c-aa1e-4f62-b1d6-292d43d640e8\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'kaggle.json': b'{\"username\":\"gungunvyas3\",\"key\":\"8fe7497dbe18ca6475b5a89094e00b9c\"}'}"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t7By68_zFxLu"
      },
      "outputs": [],
      "source": [
        "!mkdir ~/.kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GmvyMfnOGnyn"
      },
      "outputs": [],
      "source": [
        "!mv /content/kaggle.json ~/.kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TKhUds90G3dH"
      },
      "outputs": [],
      "source": [
        "!chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B2fQB7h3HqU1",
        "outputId": "1a88a5e2-166b-4c05-e54a-764f48717c40"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (1.5.16)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle) (1.16.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from kaggle) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kaggle) (4.66.2)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.0.7)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle) (6.1.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.6)\n"
          ]
        }
      ],
      "source": [
        "!pip install kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rea08jicHGmS",
        "outputId": "707e0d75-b38b-473f-f05e-f1414907531c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "401 - Unauthorized - Unauthenticated\n"
          ]
        }
      ],
      "source": [
        "!kaggle datasets download -d austinreese/craigslist-carstrucks-data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "la0ou32KoQyI"
      },
      "source": [
        "#Let's Unzip it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CWNCX595H0e9",
        "outputId": "27042f12-64c0-47a5-a790-0e67fabea4a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "unzip:  cannot find or open /content/craigslist-carstrucks-data.zip, /content/craigslist-carstrucks-data.zip.zip or /content/craigslist-carstrucks-data.zip.ZIP.\n"
          ]
        }
      ],
      "source": [
        "!unzip /content/craigslist-carstrucks-data.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Em_uOq_uM5x8"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rERWQUORoVYi"
      },
      "source": [
        "#Let's read the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q7hLAdEvSmsN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "outputId": "cd08e373-00c7-4fcd-9d8f-2bc6b69398cb"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/vehicles.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-d73cee352228>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/vehicles.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 )\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1442\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1444\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1735\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1736\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1737\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    857\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/vehicles.csv'"
          ]
        }
      ],
      "source": [
        "data = pd.read_csv(\"/content/vehicles.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eVen_4C8oZxF"
      },
      "source": [
        "#Let's have a preliminary look on our data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fcEGwF1VTQHf"
      },
      "outputs": [],
      "source": [
        "data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_1Ph-Dx4ojBy"
      },
      "source": [
        "#Let's see how many and what columns are there in our dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WZNiol6gTdKM"
      },
      "outputs": [],
      "source": [
        "data.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EMcLgzLGoslZ"
      },
      "source": [
        "#Let's see how many rows are there in our dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dQ25A037UJ0j"
      },
      "outputs": [],
      "source": [
        "data.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_miA-TcpAs9"
      },
      "source": [
        "###As we can see above that **each row of the dataset has the data of a single used car having 25 Dimensional input feature vector which is a row vector ($\\vec{x}^i$) and a target feature (reference output or correct answer, $y_i$)**, making each row of the dataset as a 26 Dimensional row vector having 25 Dimensional Input feature row vector and a scalar target feature."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bXATIcNCqmLT"
      },
      "source": [
        "#Let's determine that whether any column of our dataset has missing values or not."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ooBKv-3vU9D_"
      },
      "outputs": [],
      "source": [
        "data.isna().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HPH2FAp2qqyF"
      },
      "source": [
        "###It can be observed above that **only some columns: ```id```, ```url```, ```region```, ```region_url```, ```price``` and ```state``` has no missing values** and rest all the other columns have missing values.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58cAR9DRrRrS"
      },
      "source": [
        "###Before filling up the missing values, **let's remove or ```drop``` all the irrelevant columns which is expected to play no role** in making accurate prediction of Price of a used car."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jmJ2QjQfVHpH"
      },
      "outputs": [],
      "source": [
        "data.drop(labels=data.columns[0:4],axis=1,inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lc0hvyajYhkK"
      },
      "outputs": [],
      "source": [
        "data.drop(labels=\"title_status\",axis=1,inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qc0EJJvxa3Rh"
      },
      "outputs": [],
      "source": [
        "data.drop(labels=data.columns[14:],axis=1,inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OxCMjWcocJh5"
      },
      "outputs": [],
      "source": [
        "data.drop(labels=\"VIN\",axis=1,inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uNVvtCMpsIIY"
      },
      "source": [
        "###Above, we can see that **we have removed or ```drop```ed the following columns: ```id```, ```url```, ```region```, ```region_url```, ```title_status```, ```VIN```, ```image_url```, ```description```, ```county```, ```state```, ```lat```, ```long``` and ```posting_date``` as these columns are expected to play no role in making accurate prediction** in the Price of a used car."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0QWLUBr6mA9F"
      },
      "outputs": [],
      "source": [
        "data.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BbL-VXe8tf3x"
      },
      "source": [
        "#**After removing or ```drop```ing** all the 13 irrelevant columns from the dataset, **we are left with only 13 columns in the dataset.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jJRkoce4mwiX"
      },
      "outputs": [],
      "source": [
        "data.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ha2_VNYuumeQ"
      },
      "source": [
        "###Now, our filtered dataset has only **13 relevant columns left which has a 12 Dimensional input feature row vector ($\\vec{x}^i$) and a scalar target feature ($y_i$).**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3OeS3Sniw02_"
      },
      "source": [
        "#From this filtered dataset, **we are now going to remove or ```drop``` all the complete rows or complete data of all the used bikes for which the whole input feature row vector ($\\vec{x}^i$) is ```numpy.nan``` or ```np.nan```**, that is at maximum 12 columns in those rows are missing (NaN or ```np.nan```)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xGEUyQaRTD4a"
      },
      "outputs": [],
      "source": [
        "data.dropna(axis=0,thresh=12,inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "njW-zGVbVlNG"
      },
      "outputs": [],
      "source": [
        "data.isna().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MPQlQ-FXw-E0"
      },
      "source": [
        "###After removing or ```drop```ing the rows, we are left with less number of missing values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kCQUHWBSBr2V"
      },
      "outputs": [],
      "source": [
        "data[\"odometer\"].fillna(value=data[\"odometer\"].mean(),inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gIYRFTg3CEKK"
      },
      "outputs": [],
      "source": [
        "data[\"condition\"].fillna(value=data[\"condition\"].value_counts().index[data[\"condition\"].value_counts().argmax()],\n",
        "                         inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DjyKepnqCLje"
      },
      "outputs": [],
      "source": [
        "data[\"paint_color\"].fillna(value=data[\"paint_color\"].value_counts().index[data[\"paint_color\"].value_counts().argmax()],\n",
        "                           inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bglHoLphCmF9"
      },
      "outputs": [],
      "source": [
        "data[\"transmission\"].fillna(value=data[\"transmission\"].value_counts().index[data[\"transmission\"].value_counts().argmax()],\n",
        "                           inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-DveAOUZighn"
      },
      "outputs": [],
      "source": [
        "data[\"manufacturer\"].fillna(value=data[\"manufacturer\"].value_counts().index[data[\"manufacturer\"].value_counts().argmax()],\n",
        "                           inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3s5Re82w5ey"
      },
      "source": [
        "###As we can see above that, we have filled up missing values or **applied ```fillna``` function to fill missing values in the following columns: ```odometer```, ```condition```, ```paint_color```, ```transmission``` and ```manufacturer```.** In order to **fill up missing values in the ```odometer``` column, we have used the mean of values in that column because we are presuming that the data inside the column of ```odometer``` is Normally Distributed**, that is $\\mathcal{N}(\\mu_\\text{odometer},\\sigma^2)$ and the **missing values in the rest of columns are being filled up by the most frequently occurring values in those columns.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eVcFfwPDz6q_"
      },
      "source": [
        "#For **filling up the missing values in rest of the columns, we are going to use the Cross Tabulation Analysis because the values of these rest columns are dependent on other columns.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vLvJxXlN0n0O"
      },
      "source": [
        "#**Let's try to understand that what Cross Tabulation is?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M0aNTFLa068p"
      },
      "source": [
        "###In Cross Tabulation Analysis we create a Contingency Table, basically a table having rows and columns indexed by values of a different Single Feature or combination of values of two or more than two features present in the pandas dataframe."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-iuP2vhFuvC"
      },
      "source": [
        "###**Each cell** in this table, that is the **intersection of any column and row in the table has the count or frequency of occurrence of all the values used as index of a row as well as column of that specific cell jointly**, in the pandas dataframe. High Frequency implies High Relative Frequency which implies High Probability. Therefore, **using the Contingency table, we can easily determine that which values of different features are co-occuring most frequently or most probably and then we can use this information to fill up missing values.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uq5ziWbsCoHm"
      },
      "outputs": [],
      "source": [
        "crosstab_df_mdl_yr_mfc = pd.crosstab(data[\"model\"],\n",
        " [data[\"year\"],data[\"manufacturer\"]],rownames=[\"model\"],\n",
        "                                     colnames=[\"year\",\"manufacturer\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9hQQ-eNDPC2t"
      },
      "source": [
        "###In the code snippet above, we have created a **Contingency table with rows as different Models of used cars and columns as different years as well as manufacturers of used cars, jointly.** Let's understand this with the help of an example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0u_lrpIBM4Xc"
      },
      "outputs": [],
      "source": [
        "crosstab_df_mdl_yr_mfc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GWL_Mfw6OSfy"
      },
      "source": [
        "#Each entry in a cell shows a Frequency or Count, as shown above."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_s4ffO27oRna"
      },
      "source": [
        "#**Let's try to determine that ```if (data[\"year\"] == 2022) and (data[\"manufacturer\"] == \"toyota\")``` then what will be the most probable or most frequent ```model```.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "njU7jKVfl-65"
      },
      "outputs": [],
      "source": [
        "crosstab_df_mdl_yr_mfc.columns[-1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rSlKW_PDpajJ"
      },
      "source": [
        "#**Let's first try to determine the Frequency Distribution over different ```model```s based on the condition that ```(data[\"year\"] == 2022) and (data[\"manufacturer\"] == \"toyota\")```.**  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_lcdNrjCqZjj"
      },
      "source": [
        "#**In mathematical terms, we are trying to determine $F($ ```model```|(```year = 2022``` $\\cap$ ```manufacturer = \"toyota\"```)$)$**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJnB6qcHruki"
      },
      "source": [
        "#**And the frequency distribution, $F($ ```model```|(```year = 2022``` $\\cap$ ```manufacturer = \"toyota\"```)$)$ is:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VwMEIvbBmbGh"
      },
      "outputs": [],
      "source": [
        "freq_dist = crosstab_df_mdl_yr_mfc[crosstab_df_mdl_yr_mfc.columns[-1]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LXhLZPuAssj0"
      },
      "outputs": [],
      "source": [
        "freq_dist"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JexK9bTMubBQ"
      },
      "source": [
        "###Now, let's try to determine that **out of all the models, which model has got the highest frequency or most frequent or most probable.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f7Y34RbSrniY"
      },
      "outputs": [],
      "source": [
        "freq_dist.index[freq_dist.argmax()]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1IFYAj_vMGE"
      },
      "source": [
        "###Therefore, we can conclude that, **$F($ ```model = \"mighty max\"```|(```year = 2022``` $\\cap$ ```manufacturer = \"toyota\"```)$)$ is highest among all the other Frequencies.** Therefore, **everytime in our data whenever in any row, ```(year == 2022) and (manufacturer == \"toyota\")``` and if missing value is there in the ```model``` of that row then we are going to fill up that missing value with ```\"mighty max\"```.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "na1e9Zt_2jQm"
      },
      "source": [
        "###**Note:** Actually we are computing $F($ ```model = \"mighty max\"``` $\\cap$ ```year = 2022``` $\\cap$ ```manufacturer = \"toyota\"```$)$ instead of computing $F($ ```model = \"mighty max\"```|(```year = 2022``` $\\cap$ ```manufacturer = \"toyota\"```)$)$ because from the equation of Conditional Probability, we know that:\n",
        "\n",
        "\\begin{equation}\n",
        "P(A|B) = \\frac{P(A \\cap B)}{P(B)}\n",
        "\\end{equation}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3FX5J_q4SxU"
      },
      "source": [
        "###Following on the same definition of the Conditional Probability, we can say that:\n",
        "\n",
        "#$P(\\text{model} | \\text{year} \\cap \\text{manufacturer}) = \\frac{P(\\text{model} \\cap \\text{year} \\cap \\text{manufacturer})}{P(\\text{year} \\cap \\text{manufacturer})}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SSqRrxvE7eHq"
      },
      "source": [
        "###But the fact is that, **$P(\\text{year} \\cap \\text{manufacturer})$ remains constant** and therefore:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJvZs6Zt8xfa"
      },
      "source": [
        "$P(\\text{model} | \\text{year} \\cap \\text{manufacturer}) ∝ P(\\text{model} \\cap \\text{year} \\cap \\text{manufacturer})$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-e9GRFqx86xm"
      },
      "source": [
        "#OR"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9sJzoz79BqB"
      },
      "source": [
        "$F(\\text{model} | \\text{year} \\cap \\text{manufacturer}) ∝ F(\\text{model} \\cap \\text{year} \\cap \\text{manufacturer})$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_KiJE9N9IxN"
      },
      "source": [
        "###**Therefore, whether we compute the highest $F(\\text{model} | \\text{year} \\cap \\text{manufacturer})$ or $F(\\text{model} \\cap \\text{year} \\cap \\text{manufacturer})$, it's the same thing, we will get the same value of ```model``` for ```year``` $\\cap$ ```manufacturer```.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6PqrIC539q-g"
      },
      "source": [
        "###**And, it's easier to compute $F(\\text{model} \\cap \\text{year} \\cap \\text{manufacturer})$ rather than computing $F(\\text{model} | \\text{year} \\cap \\text{manufacturer})$, therefore we decided to compute $F(\\text{model} \\cap \\text{year} \\cap \\text{manufacturer})$**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5BjExQGTwxRL"
      },
      "source": [
        "###In a similar way **we can determine the highest occurring ```model``` for the combination of each ```year``` as well as ```manufacturer```, that is ```(year``` $\\cap$ ```manufacturer)```**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0LclX7BdDM6J"
      },
      "outputs": [],
      "source": [
        "mapping_dict = dict()\n",
        "\n",
        "for single_col in crosstab_df_mdl_yr_mfc.columns:\n",
        "  mapping_dict[single_col] = crosstab_df_mdl_yr_mfc[single_col].index[crosstab_df_mdl_yr_mfc[single_col].argmax()]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nFi4F1dIxZT4"
      },
      "source": [
        "#The code snippet above is actually maintaining the dictionary of most frequent occuring ```model``` for ```(year``` $\\cap$ ```manufacturer)```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s72vpu7hFXNO"
      },
      "outputs": [],
      "source": [
        "for k in mapping_dict.keys():\n",
        "  boolean_mask = (data[\"year\"] == k[0]) & (data[\"manufacturer\"] == k[1])\n",
        "  data.loc[boolean_mask,\"model\"] = data.loc[boolean_mask,\"model\"].fillna(value=mapping_dict[k],inplace=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_D2LH-1x6db"
      },
      "source": [
        "#The code snippet above is using the dictionary maintained to fill up the missing values in the rows having specific ```year``` as well as ```manufacturer```."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xei59aWJezzj"
      },
      "outputs": [],
      "source": [
        "data[\"model\"].fillna(value=data[\"model\"].value_counts().index[data[\"model\"].value_counts().argmax()],\n",
        "                           inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-5kYVjqyS_w"
      },
      "source": [
        "#Even after filling up missing values using the dictionary, **still some missing values are left and therefore we are simply going to fill up them with the most frequently occurring ```model``` irrespective of ```(year``` $\\cap$ ```manufacturer)```**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QYVIjN4qjLFO"
      },
      "outputs": [],
      "source": [
        "data[\"cylinders\"].fillna(value=data[\"cylinders\"].value_counts().index[data[\"cylinders\"].value_counts().argmax()],\n",
        "                           inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abZKM6PO0r5r"
      },
      "source": [
        "#As the **values in ```cylinder``` column usually doesn't depends upon the values in any other column and therefore, we are not going to create any kind of contingency table in this case** and simply fill up all the missing values with the most frequently occurring value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AY-3HPNdkmsx"
      },
      "outputs": [],
      "source": [
        "crosstab_df_type_mfc = pd.crosstab(data[\"type\"],\n",
        "                                       data[\"manufacturer\"],rownames=[\"type\"],\n",
        "                                     colnames=[\"manufacturer\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_uE0OFAFlK4l"
      },
      "outputs": [],
      "source": [
        "mapping_dict = dict()\n",
        "\n",
        "for single_col in crosstab_df_type_mfc.columns:\n",
        "  mapping_dict[single_col] = crosstab_df_type_mfc[single_col].index[crosstab_df_type_mfc[single_col].argmax()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tIjxN3eolfEK"
      },
      "outputs": [],
      "source": [
        "for k in mapping_dict.keys():\n",
        "  boolean_mask = (data[\"manufacturer\"] == k)\n",
        "  data.loc[boolean_mask,\"type\"] = data.loc[boolean_mask,\"type\"].fillna(value=mapping_dict[k],inplace=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L_sZdpYio4XZ"
      },
      "outputs": [],
      "source": [
        "crosstab_df_drive_mfc_mdl = pd.crosstab(data[\"drive\"],\n",
        "                                       [data[\"manufacturer\"], data[\"model\"]],rownames=[\"drive\"],\n",
        "                                     colnames=[\"manufacturer\",\"model\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0qIc1uhHpafH"
      },
      "outputs": [],
      "source": [
        "mapping_dict = dict()\n",
        "\n",
        "for single_col in crosstab_df_drive_mfc_mdl.columns:\n",
        "  mapping_dict[single_col] = crosstab_df_drive_mfc_mdl[single_col].index[crosstab_df_drive_mfc_mdl[single_col].argmax()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gm9aJfKWpu7Y"
      },
      "outputs": [],
      "source": [
        "for k in mapping_dict.keys():\n",
        "  boolean_mask = (data[\"manufacturer\"] == k[0]) & (data[\"model\"] == k[1])\n",
        "  data.loc[boolean_mask,\"drive\"] = data.loc[boolean_mask,\"drive\"].fillna(value=mapping_dict[k],inplace=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yuT6x7dZrn78"
      },
      "outputs": [],
      "source": [
        "data[\"drive\"].fillna(value=data[\"drive\"].value_counts().index[data[\"drive\"].value_counts().argmax()],\n",
        "                           inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rDr0tyfnsCMa"
      },
      "outputs": [],
      "source": [
        "crosstab_df_size_mfc_mdl = pd.crosstab(data[\"size\"],\n",
        "                                       [data[\"manufacturer\"], data[\"model\"]],rownames=[\"size\"],\n",
        "                                     colnames=[\"manufacturer\",\"model\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yGrsdLnlsPpy"
      },
      "outputs": [],
      "source": [
        "mapping_dict = dict()\n",
        "\n",
        "for single_col in crosstab_df_size_mfc_mdl.columns:\n",
        "  mapping_dict[single_col] = crosstab_df_size_mfc_mdl[single_col].index[crosstab_df_size_mfc_mdl[single_col].argmax()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LoiTM47asjJN"
      },
      "outputs": [],
      "source": [
        "for k in mapping_dict.keys():\n",
        "  boolean_mask = (data[\"manufacturer\"] == k[0]) & (data[\"model\"] == k[1])\n",
        "  data.loc[boolean_mask,\"size\"] = data.loc[boolean_mask,\"size\"].fillna(value=mapping_dict[k],inplace=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b-C2faEyuVS9"
      },
      "outputs": [],
      "source": [
        "data[\"size\"].fillna(value=data[\"size\"].value_counts().index[data[\"size\"].value_counts().argmax()],\n",
        "                           inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-gmNoM_bzPY7"
      },
      "source": [
        "###In the code snippets above, **we are filling up the missing values in:**\n",
        "\n",
        "1. ```type``` column based on ```manufacturer``` by creating their contingency table.\n",
        "\n",
        "2. ```drive``` column based on ```(manufacturer``` $\\cap$ ```model)``` by creating their contingency table.\n",
        "\n",
        "3. ```size``` column based on ```(manufacturer``` $\\cap$ ```model)``` by creating their contingency table."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "wUsRp-fwv7dH",
        "outputId": "4ee8dbe8-77b7-423a-93d1-496c3834542e"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'data' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-811a2046da5d>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
          ]
        }
      ],
      "source": [
        "data.isna().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1N-UfwRb1DfC"
      },
      "source": [
        "#As we can see above that we are not left with any missing values in any column."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AIuk1PKx1Ovw"
      },
      "source": [
        "#Finally, let's write the cleaned data into another ```.csv``` file **so that for training the Linear Regression model, we can directly read this file, ignoring to repeat all the above steps which we did before to clean the data.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E5lVt6yAueQi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "outputId": "72d70573-03e0-406c-c006-2f4a1229ee76"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'data' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-bf5ebff909e1>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/cleaned_vehicles.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
          ]
        }
      ],
      "source": [
        "data.to_csv(\"/content/cleaned_vehicles.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "icR6PADR1sC_"
      },
      "source": [
        "#Now, **let's copy this file to the google drive** so that it can be retained anytime whenever needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sKUMKMl1unPH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d44ddebd-66d8-48a0-d19e-e17ddac34ac2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cp: cannot stat '/content/cleaned_vehicles.csv': No such file or directory\n"
          ]
        }
      ],
      "source": [
        "!cp /content/cleaned_vehicles.csv /content/drive/MyDrive/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJ8cMgj5sGA_"
      },
      "source": [
        "##Let's copy the file of the cleaned data from google drive back into the local drive and **encode categorical values either into integers or One Hot Encoding.**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "Utc6OJVIhvXn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eKJNnyItvSIZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "460e84ca-67f9-449d-e8df-3c5a798219ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cp: cannot stat '/content/drive/MyDrive/cleaned_vehicles.csv': No such file or directory\n"
          ]
        }
      ],
      "source": [
        "!cp /content/drive/MyDrive/cleaned_vehicles.csv /content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BeJ1iK6xtpI1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "outputId": "7770233c-8a26-4ef8-e8d2-93ae7d62485e"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/cleaned_vehicles.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-4c649c24b070>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/cleaned_vehicles.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 )\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1442\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1444\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1735\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1736\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1737\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    857\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/cleaned_vehicles.csv'"
          ]
        }
      ],
      "source": [
        "data = pd.read_csv(\"/content/cleaned_vehicles.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VHrTlEuIt0eL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "outputId": "e877cd33-bdd3-40fd-948a-0d30af81a5f6"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'data' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-f36c0695f10d>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
          ]
        }
      ],
      "source": [
        "data = data.iloc[:,1:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "9r3vsowht3JM",
        "outputId": "f854187e-888c-42ac-cc9c-bca29c9fefba"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'data' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-304fa4ce4ebd>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
          ]
        }
      ],
      "source": [
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qiNyNKGkt8pv"
      },
      "outputs": [],
      "source": [
        "data.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2dxl3DlWuVcF"
      },
      "source": [
        "#As we can see that now the cleaned data has the data of 144017 Used cars."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Let's determine which columns are Categorical so that we can encode them into Integer or One Hot Encoded Values.**"
      ],
      "metadata": {
        "id": "o-tDwyZMi_TS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "categorical_columns = list()\n",
        "\n",
        "for single_column in data.columns:\n",
        "\n",
        "  if data[single_column].dtype == \"object\":\n",
        "    categorical_columns.append(single_column)"
      ],
      "metadata": {
        "id": "XkW4VHc2i4kO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(categorical_columns)"
      ],
      "metadata": {
        "id": "pY6v786hkddV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Let's first encode the categorical values of **all those columns which are Ordinal or their values have some Order associated with them and hence they can be compared**, such as the column of ```cylinders``` which can be compared such as ```3 cylinders``` or ```4 cylinders```. So, the following columns are Ordinal:\n",
        "\n",
        "###```condition```\n",
        "###```cylinders```\n",
        "###```transmission```\n",
        "###```drive```\n",
        "###```size```\n",
        "\n",
        "###Rest of the columns are Nominal.\n",
        "\n",
        "##Therefore, **categorical columns are of two types: Nominal (having no order associated with their values) as well as Ordinal (having some order associated with them).**\n"
      ],
      "metadata": {
        "id": "52t2cDtCk24i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Let's encode the values of Ordinal Columns now.**"
      ],
      "metadata": {
        "id": "Pdl1634dosd-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###But, first let us determine that what are the unique values available in all the Ordinal Columns."
      ],
      "metadata": {
        "id": "lZmvXE7dsWmF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for single_column in [\"condition\",\"cylinders\",\"transmission\",\"drive\",\"size\"]:\n",
        "\n",
        "  print(\"The unique values in {} column are {}\".format(single_column,\n",
        "                                                       data[single_column].unique()))"
      ],
      "metadata": {
        "id": "6Ixz5cI3r065"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Let's create the dictionary of mappings for all the categorical values to Integers, in all the Ordinal Columns."
      ],
      "metadata": {
        "id": "S7KkcjiWbSFP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "condition_encoding_dict = {'excellent':6, 'good':3, 'like new':4, 'new':5, 'fair':2, 'salvage':1}\n",
        "cylinders_encoding_dict = {'6 cylinders':6, '8 cylinders':8, '4 cylinders':4, '5 cylinders':5,\n",
        "                           '10 cylinders':10, '3 cylinders':3, 'other':7, '12 cylinders':12}\n",
        "transmission_encoding_dict = {'automatic':3, 'other':2, 'manual':1}\n",
        "drive_encoding_dict = {'rwd':2, '4wd':3, 'fwd':1}\n",
        "size_encoding_dict = {'full-size':4, 'mid-size':3, 'compact':1, 'sub-compact':2}"
      ],
      "metadata": {
        "id": "4p5ZZtOquJWq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Let's replace all the categorical values in all the Ordinal columns, with their respective integer mappings."
      ],
      "metadata": {
        "id": "cNFHHtW5cMod"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data[\"condition\"].replace(to_replace=condition_encoding_dict.keys(),\n",
        "                          value=condition_encoding_dict.values(),inplace=True)\n",
        "\n",
        "data[\"cylinders\"].replace(to_replace=cylinders_encoding_dict.keys(),\n",
        "                          value=cylinders_encoding_dict.values(),inplace=True)\n",
        "\n",
        "data[\"transmission\"].replace(to_replace=transmission_encoding_dict.keys(),\n",
        "                             value=transmission_encoding_dict.values(),inplace=True)\n",
        "\n",
        "data[\"drive\"].replace(to_replace=drive_encoding_dict.keys(),\n",
        "                      value=drive_encoding_dict.values(),inplace=True)\n",
        "\n",
        "data[\"size\"].replace(to_replace=size_encoding_dict.keys(),\n",
        "                     value=size_encoding_dict.values(),inplace=True)"
      ],
      "metadata": {
        "id": "WTAyT2hLzAuK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Let's now encode all the Nominal Columns with the help of One Hot Encoding.**"
      ],
      "metadata": {
        "id": "jiMRv0BO0obu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Let's first that see how many unique values are there in each of the Nominal Columns."
      ],
      "metadata": {
        "id": "mqYiQAqG0-2W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for single_column in [\"manufacturer\",\"model\",\"fuel\",\"type\",\"paint_color\"]:\n",
        "\n",
        "  print(\"The number of unique values in {} column are {}\".format(single_column,\n",
        "                                                       data[single_column].unique().shape[0]))"
      ],
      "metadata": {
        "id": "Iai79JNi1pT8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**As all the Nominal columns except, ```model``` have very less number of unique values so we can One Hot Encode all the values of rest of the columns.**"
      ],
      "metadata": {
        "id": "VmCAh3hhHxg7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Let's see what can we do with the values of ```model``` column."
      ],
      "metadata": {
        "id": "3TBBpGFWIuHb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Let's first encode all the unique values in the ```model``` column and visualize the frequency distribution of the integer values in ```model``` column.**"
      ],
      "metadata": {
        "id": "zCw40W-QJBTG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data[\"model\"].replace(to_replace=data[\"model\"].unique(),\n",
        "                      value=list(range(data[\"model\"].unique().shape[0])),\n",
        "                      inplace=True)"
      ],
      "metadata": {
        "id": "6NxGxRXAI2dI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.hist(data[\"model\"])\n",
        "plt.xlabel(\"Integer Encodings of Different Models of Different Cars\")\n",
        "plt.ylabel(\"Frequency of Integer Labels\")"
      ],
      "metadata": {
        "id": "tmx_aN-SK7KQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##As can be visualized that **all the integer mappings roughly below 2000 are occupying majority of the area under the Frequency Distribution** and thats the piece of information which we are going to use **to determine the various integer mappings that need to be One Hot Encoded and rest of the Integer mappings will be merged into a single integer mapping.**   "
      ],
      "metadata": {
        "id": "hgSzkDHOPkcw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**We will One Hot Encode only those top 'k' (Top 'k' most frequently occuring)Integer mappings seperately which are contributing to majority (let's say 70%) of the area under the curve of Frequency Distribution and merge rest of them into a single integer value.**"
      ],
      "metadata": {
        "id": "72IdpIQihwSy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "models_freq_dist = dict(data[\"model\"].value_counts())"
      ],
      "metadata": {
        "id": "7J16ADjqRFZE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##And the best part is that all the integer mappings we computed above are by default arranged in descending order according to their frequencies and **we simply have to get the list of all the integer mappings whose frequencies when summed up are contributing to 70% of the area under the Frequency Distribution.**"
      ],
      "metadata": {
        "id": "CfFrHZWriyzD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "area_cut_off = 0.7\n",
        "top_models = list()\n",
        "total_area = 0\n",
        "total_frequency = data.shape[0]\n",
        "\n",
        "for k,v in models_freq_dist.items():\n",
        "\n",
        "  if (total_area/total_frequency) > area_cut_off:\n",
        "    break\n",
        "\n",
        "  top_models.append(k)\n",
        "\n",
        "  total_area += models_freq_dist[k]"
      ],
      "metadata": {
        "id": "ODtIA3eoRPno"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "k = len(top_models)\n",
        "print(k)"
      ],
      "metadata": {
        "id": "A0Sdjql6kr-S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Therefore, it can be said that there are 773 top most frequently occurring models whose frequencies when summed up contribute to 70% of the area under the Frequency Distribution.**  "
      ],
      "metadata": {
        "id": "K0obDaGhlP59"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Let's assign a unique integer value to each of the integer mappings till index 772 (k-1) and then after that because we need to merge all the integer mappings, assign the index 773 to all of them."
      ],
      "metadata": {
        "id": "zlFD0EuUo2M-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "updated_mapping_dict = dict()\n",
        "\n",
        "for i,item in enumerate(models_freq_dist.items()):\n",
        "\n",
        "  if i <= (k-1):\n",
        "    updated_mapping_dict[item[0]] = i\n",
        "  else:\n",
        "    updated_mapping_dict[item[0]] = k"
      ],
      "metadata": {
        "id": "a3BRQNx8mEHQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data[\"model\"].replace(to_replace=updated_mapping_dict.keys(),\n",
        "                       value=updated_mapping_dict.values(),inplace=True)"
      ],
      "metadata": {
        "id": "Cc9h26b0q-p1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "id": "ifN-2yT5rdcG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Let's finally concatenate the 774 columns of One Hot Encodings of integer mappings available in ```model``` column. For that let's create a function which will serve as a generic function for One Hot Encoding the integer mappings in rest of the columns too."
      ],
      "metadata": {
        "id": "kHbf2JC3tQdq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_to_ohe_df(column_name):\n",
        "\n",
        "  model_unique_values = data[column_name].unique().shape[0]\n",
        "  identity_matrix = np.eye(model_unique_values,model_unique_values)\n",
        "  ohe_models = identity_matrix[data[column_name]]\n",
        "  df_column_names = [column_name+str(i) for i in range(model_unique_values)]\n",
        "  column_df = pd.DataFrame(data=ohe_models,columns=df_column_names)\n",
        "\n",
        "  return column_df"
      ],
      "metadata": {
        "id": "wvlJZ9Amtwpb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Let's Integer Encode remaining columns so that we can One Hot Encode them."
      ],
      "metadata": {
        "id": "qh2AkcQcsDmC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for single_column in [\"manufacturer\",\"fuel\",\"type\",\"paint_color\"]:\n",
        "\n",
        "  column_unique_values = data[single_column].unique()\n",
        "  data[single_column].replace(to_replace=column_unique_values,\n",
        "                              value=list(range(column_unique_values.shape[0])),\n",
        "                              inplace=True)"
      ],
      "metadata": {
        "id": "89t9kk8Qwf0-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "id": "Sl0pxqIUxp69"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for single_column in [\"manufacturer\",\"model\",\"fuel\",\"type\",\"paint_color\"]:\n",
        "\n",
        "  column_df = convert_to_ohe_df(single_column)\n",
        "  data.drop(labels=single_column,axis=1,inplace=True)\n",
        "  data = pd.concat([data,column_df],axis=1)"
      ],
      "metadata": {
        "id": "csWU1nnRs000"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "id": "SE5GY1IG1K78"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Let's also transform the ```year``` column of the pandas dataframe into something which represents how much old the car is by subtracting year of the oldest car in the dataset from the year of each car** or by subtracting the year when the first commercial car was introduced, that is 1886."
      ],
      "metadata": {
        "id": "o086ij775bOz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data[\"year\"] = data[\"year\"] - 1886"
      ],
      "metadata": {
        "id": "X5SIxxMy5agq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "id": "eenEAsWp6iLw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.to_csv(\"/content/updated_cleaned_vehicles.csv\",index=False)"
      ],
      "metadata": {
        "id": "p1BcDABn1dLN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/updated_cleaned_vehicles.csv /content/drive/MyDrive/updated_cleaned_vehicles.csv"
      ],
      "metadata": {
        "id": "yaewv_zB2Vgb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/drive/MyDrive/updated_cleaned_vehicles.csv /content/updated_cleaned_vehicles.csv"
      ],
      "metadata": {
        "id": "ckraAvn72pUl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Let's filter out those input feature columns which have high pearson correlation with the ```price``` column.  "
      ],
      "metadata": {
        "id": "nrpGYMoN3MGw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(\"/content/updated_cleaned_vehicles.csv\")"
      ],
      "metadata": {
        "id": "nfAxyrerF6St"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pearson_corr = list()\n",
        "\n",
        "for single_column in data.columns:\n",
        "  pearson_corr.append(data[\"price\"].corr(data[single_column]))\n",
        "\n",
        "relevant_columns_idx = np.argwhere(np.abs(np.array(pearson_corr))>10**(-3))"
      ],
      "metadata": {
        "id": "O6em3ezj3z1O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "relevant_columns_idx[:,0]"
      ],
      "metadata": {
        "id": "web6z8VZE8Ot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = data.iloc[:,relevant_columns_idx[:,0]]"
      ],
      "metadata": {
        "id": "sgkJu1hEFbkE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "id": "nl4tipYBFnbz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.heatmap(data.corr(),xticklabels=True,yticklabels=True)\n",
        "plt.figure(figsize=(25,25))"
      ],
      "metadata": {
        "id": "_BBR_SxUGyDi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_pca(X,preserved_variance_percentage):\n",
        "\n",
        "  unprojected_X = np.array(X)\n",
        "  X_cov = np.array(X.cov())\n",
        "  Q_lambda_QT = np.linalg.svd(X_cov)\n",
        "  total_variance = np.sum(Q_lambda_QT[1])\n",
        "  variance_sum = 0\n",
        "  num_eigen_vectors = 1\n",
        "\n",
        "  if preserved_variance_percentage == 1:\n",
        "    projected_X = np.matmul(unprojected_X,Q_lambda_QT[0])\n",
        "    projected_X = pd.DataFrame(data=projected_X,\n",
        "                             columns=[\"feature_\"+str(i) for i in range(X.shape[1])])\n",
        "  else:\n",
        "    for variance in Q_lambda_QT[1]:\n",
        "\n",
        "      if (variance_sum/total_variance) > preserved_variance_percentage:\n",
        "        break\n",
        "\n",
        "      variance_sum += variance\n",
        "      num_eigen_vectors += 1\n",
        "\n",
        "    projected_X = np.matmul(unprojected_X,Q_lambda_QT[0][0:num_eigen_vectors])\n",
        "\n",
        "    projected_X = pd.DataFrame(data=projected_X,\n",
        "                             columns=[\"feature_\"+str(i) for i in range(num_eigen_vectors)])\n",
        "\n",
        "  return projected_X"
      ],
      "metadata": {
        "id": "4MQzCEZaEHeP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "projected_X = apply_pca(data.iloc[:,1:],1)"
      ],
      "metadata": {
        "id": "X_iZC5MQDqw2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "projected_X.head()"
      ],
      "metadata": {
        "id": "ovOMhcQsKEYk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "projected_data = pd.concat([projected_X,data[\"price\"]],axis=1)"
      ],
      "metadata": {
        "id": "88nmebCwKUo8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "projected_data.head()"
      ],
      "metadata": {
        "id": "az3KkdoNKvcJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.heatmap(projected_data.corr(),xticklabels=True,yticklabels=True)\n",
        "plt.figure(figsize=(25,25))"
      ],
      "metadata": {
        "id": "hjXHKaGOKSc0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VfoUuUx2u7uR"
      },
      "source": [
        "##Now, we are going to split this data into Training and Testing Data with 70 % of the rows in Training Data and remaining 30 % in the Testing Data. Therefore, $N_\\text{train} = 0.7 \\times 144017 = 100811$ and $N_\\text{test} = (144017 - 100811) = 43206$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nWF6aw89uCFi"
      },
      "outputs": [],
      "source": [
        "training_data_len = int(0.7 * projected_data.shape[0])\n",
        "training_data = projected_data.iloc[0:training_data_len,:]\n",
        "\n",
        "testing_data = projected_data.iloc[training_data_len:,:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sH7UEyUFxLoQ"
      },
      "outputs": [],
      "source": [
        "training_data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V5rQjRauytfU"
      },
      "outputs": [],
      "source": [
        "testing_data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "urX0m5TZyvmF"
      },
      "outputs": [],
      "source": [
        "training_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MEJej7D2zKoj"
      },
      "outputs": [],
      "source": [
        "X_train_transpose = np.array(training_data.iloc[:,0:projected_data.shape[1]-1])\n",
        "y_train = np.array(training_data[\"price\"]).reshape(-1,1)\n",
        "\n",
        "X_test_transpose = np.array(testing_data.iloc[:,0:projected_data.shape[1]-1])\n",
        "y_test = np.array(testing_data[\"price\"]).reshape(-1,1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ia9fmISK0Dfc"
      },
      "outputs": [],
      "source": [
        "X_train_transpose.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zAofSUPW2GOK"
      },
      "outputs": [],
      "source": [
        "X_train_transpose"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CLNwyUsl2dPd"
      },
      "source": [
        "##The dataset has finally the following Input Features:\n",
        "\n",
        "##$1.$ **```manufacturer3```** $(x_1)$\n",
        "##$2.$ **```model30```** $(x_2)$\n",
        "##$3.$ **```model31```** $(x_3)$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KS7w9qum4uz-"
      },
      "source": [
        "#The Input feature row vector of any $i^{th}$ row of the data in this case is given as:\n",
        "#\\begin{equation}\n",
        "\\vec{x}^i =\n",
        "\\begin{bmatrix}\n",
        "x_1^i \\cdots x_j^i \\cdots x_{3}^i\n",
        "\\end{bmatrix}\n",
        "\\end{equation}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "es4H8sWN4Lbh"
      },
      "source": [
        "#Therefore,\n",
        "#\\begin{equation}\n",
        "X_\\text{train}^T =\n",
        "\\begin{bmatrix}\n",
        "\\longleftarrow \\vec{x}^1 \\longrightarrow \\\\\n",
        "\\vdots \\\\\n",
        "x_1^i \\cdots x_j^i \\cdots x_{3}^i \\\\\n",
        "\\vdots \\\\\n",
        "\\longleftarrow \\vec{x}^{N_\\text{train}} \\longrightarrow\n",
        "\\end{bmatrix}\n",
        "\\end{equation}\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DcHNLN_r0O2I"
      },
      "outputs": [],
      "source": [
        "y_train.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q3T6g7Gh10w_"
      },
      "source": [
        "#And,\n",
        "#\\begin{equation}\n",
        "\\begin{bmatrix}\n",
        "y_1 \\\\\n",
        "\\vdots \\\\\n",
        "y_i \\\\\n",
        "\\vdots \\\\\n",
        "y_{N_\\text{train}}\n",
        "\\end{bmatrix}\n",
        "\\end{equation}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9rce7LON8Xpi"
      },
      "source": [
        "#Now, we will be **training a Linear Regression Model on our training data using Gradient Descent Algorithm** using the following update in a **while or for loop until this update rule converges to the Minima of the $MSE$, that is $(\\theta_0^*, \\vec{\\theta}^*)$** also known as Loss Function:\n",
        "\n",
        "#\\begin{equation}\n",
        "\\theta_{final} = \\theta_{initial} - \\epsilon \\cdot \\frac{\\partial MSE}{\\partial \\theta}\n",
        "\\end{equation}\n",
        "\n",
        "#Where,\n",
        "$\\frac{\\partial MSE}{\\partial \\theta_0}$\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "N_train = y_train.shape[0]"
      ],
      "metadata": {
        "id": "zcpWZiz2H6YA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Bx9lkTh0TAa"
      },
      "outputs": [],
      "source": [
        "def del_by_del_thetas(theta_0,theta):\n",
        "\n",
        "  y_pred = (theta_0 + np.matmul(X_train_transpose,theta))\n",
        "  error_vector_transpose = np.transpose(y_pred - y_train)\n",
        "\n",
        "  del_by_del_theta = (2/N_train)*np.transpose(np.matmul(error_vector_transpose,\n",
        "                                                        X_train_transpose))\n",
        "  del_by_del_theta_0 = (2/N_train)*np.sum(error_vector_transpose)\n",
        "\n",
        "  return [del_by_del_theta_0,del_by_del_theta]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def mae(theta_0,theta):\n",
        "\n",
        "  y_pred = (theta_0 + np.matmul(X_train_transpose,theta))\n",
        "  abs_error_vector = np.abs(y_pred - y_train)\n",
        "\n",
        "  return np.mean(abs_error_vector)"
      ],
      "metadata": {
        "id": "HshNZToRwxDW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Now, Finally we are going **train Linear Regression Model using Gradient Descent Algorithm.**"
      ],
      "metadata": {
        "id": "AqqsvrtWf2xc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epsilon = 10**(-12)\n",
        "tol = 10**(-3)\n",
        "theta_0_initial = 0\n",
        "theta_initial = np.zeros((X_train_transpose.shape[1],1))\n",
        "iteration = 0\n",
        "\n",
        "while True:\n",
        "\n",
        "  initial_gradients = del_by_del_thetas(theta_0_initial,\n",
        "                                        theta_initial)\n",
        "\n",
        "  theta_0_final = theta_0_initial - (epsilon * initial_gradients[0])\n",
        "  theta_final = theta_initial - (epsilon * initial_gradients[1])\n",
        "\n",
        "  initial_gradient_vector = np.concatenate((np.array([[initial_gradients[0]]]),\n",
        "                                           initial_gradients[1]))\n",
        "\n",
        "  if np.linalg.norm(initial_gradient_vector) == 0:\n",
        "    break\n",
        "\n",
        "  initial_mae = mae(theta_0_initial,theta_initial)\n",
        "\n",
        "  print(\"The Value of MAE at iteration # {} is {}\".format(iteration,initial_mae))\n",
        "\n",
        "  theta_0_initial = theta_0_final\n",
        "  theta_initial = theta_final\n",
        "\n",
        "  iteration += 1"
      ],
      "metadata": {
        "id": "d2QVt14xfc_7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "due to changing the value from -15 to -12 the value of MAE is  now 26398"
      ],
      "metadata": {
        "id": "jV6HUjGefabM"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "s3CRcZAJh9pJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}